{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deep_translator import GoogleTranslator\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate foreign language texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code works the fastest and is the most accurate\n",
    "# however, the average time to translate text is ~8 mins\n",
    "def translate_file(file_path, target_language='en'):\n",
    "    translated_lines = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        for i in range(0, len(lines), 10):\n",
    "            batch_lines = lines[i:i+10]\n",
    "            translated_batch = GoogleTranslator(source='auto', target=target_language).translate_batch(batch_lines)\n",
    "            translated_lines.extend(translated_batch)\n",
    "            for translated_line in translated_batch:\n",
    "                print(translated_line)\n",
    "    return translated_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\T'\n",
      "C:\\Users\\sebgr\\AppData\\Local\\Temp\\ipykernel_20068\\1555076886.py:6: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  output_directory = 'RandomManifestos\\Translations'\n"
     ]
    }
   ],
   "source": [
    "#Now the above with saving to files\n",
    "def translate_file_and_save(file_path, target_language='en'):\n",
    "    translated_lines = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        output_directory = 'RandomManifestos\\Translations'\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "        output_file_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            for i in range(0, len(lines), 10):\n",
    "                batch_lines = lines[i:i+10]\n",
    "                translated_batch = GoogleTranslator(source='auto', target=target_language).translate_batch(batch_lines)\n",
    "                translated_lines.extend(translated_batch)\n",
    "                for translated_line in translated_batch:\n",
    "                    output_file.write(translated_line + '\\n')\n",
    "    return translated_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated '92022.txt' and saved to 'Translate/92022.txt'\n",
      "Translated '96630.txt' and saved to 'Translate/96630.txt'\n"
     ]
    }
   ],
   "source": [
    "directory = \"RandomManifestos/\"\n",
    "for file_name in os.listdir(directory):\n",
    "    if file_name.endswith('.txt'):\n",
    "        input_file_path = os.path.join(directory, file_name)\n",
    "        translate_file_and_save(input_file_path)\n",
    "        print(f\"Translated '{file_name}' and saved to 'Translate/{file_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dl_translate as dlt\n",
    "# This code is super slow, cant take hours to analyze a single file\n",
    "def translate_file(file_path, target_language=dlt.lang.ENGLISH):\n",
    "    translated_lines = []\n",
    "    mt = dlt.TranslationModel(\"nllb200\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        for i in range(0, len(lines), 10):\n",
    "            batch_lines = lines[i:i+10]\n",
    "            translated_batch = [mt.translate(line, source=dlt.lang.CROATIAN, target=target_language) for line in batch_lines]\n",
    "            translated_lines.extend(translated_batch)\n",
    "            for translated_line in translated_batch:\n",
    "                print(translated_line)\n",
    "    return translated_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate_file(\"2015-Country-Manifestos\\Croatia\\81032.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Uni (Python 3.12.1)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model 'llama2' not found, try pulling it first",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mollama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[0;32m      3\u001b[0m message \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhy is the sky blue?\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllama2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\ollama\\_client.py:177\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    174\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;241m:=\u001b[39m message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    175\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [_encode_image(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\ollama\\_client.py:97\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[1;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[0;32m     92\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m     94\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[1;32m---> 97\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\ollama\\_client.py:73\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m   response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mResponseError\u001b[0m: model 'llama2' not found, try pulling it first"
     ]
    }
   ],
   "source": [
    "#Now using the Ollama local LLM\n",
    "from ollama import Client\n",
    "message = {'role': 'user', 'content': 'Why is the sky blue?'}\n",
    "response = Client().chat(model='llama2', messages=[message])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Vadar to get sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None  # Handle error or unknown language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "\n",
    "text = Text(your_text_here, hint_language_code='es')  # Example for Spanish\n",
    "sentiment = text.polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from polyglot.text import Text\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to perform sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    polyglot_text = Text(text)\n",
    "    return polyglot_text.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the text file\n",
    "file_path = '2015-Country-Manifestos\\Spain\\33020.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Detecting the language\n",
    "language = detect_language(text)\n",
    "print(f\"Detected language: {language}\")\n",
    "\n",
    "# If a language is detected, perform sentiment analysis\n",
    "if language:\n",
    "    sentiment_score = analyze_sentiment(text)\n",
    "    print(f\"Sentiment score: {sentiment_score}\")\n",
    "else:\n",
    "    print(\"Could not detect language or language is not supported for sentiment analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from textblob import TextBlob\n",
    "from googletrans import Translator, LANGUAGES\n",
    "\n",
    "# Initialize the translator\n",
    "translator = Translator()\n",
    "\n",
    "def detect_language(text):\n",
    "    # Use googletrans to detect the language\n",
    "    return translator.detect(text).lang\n",
    "\n",
    "def translate_to_english(text, src_lang):\n",
    "    # If the text is already in English, no translation is needed\n",
    "    if src_lang == \"en\":\n",
    "        return text\n",
    "    # Translate the text to English\n",
    "    translated_text = translator.translate(text, src=src_lang, dest='en').text\n",
    "    return translated_text\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    # Perform sentiment analysis using TextBlob\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment\n",
    "\n",
    "def process_files(directory):\n",
    "    # List all txt files in the directory\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    \n",
    "    for file in files:\n",
    "        filepath = os.path.join(directory, file)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            lang = detect_language(text)\n",
    "            print(f\"Processing '{file}' in {LANGUAGES.get(lang, 'Unknown')}...\")\n",
    "            \n",
    "            # Translate to English if necessary\n",
    "            english_text = translate_to_english(text, lang)\n",
    "            sentiment = sentiment_analysis(english_text)\n",
    "            print(f\"Sentiment of '{file}': {sentiment}\\n\")\n",
    "\n",
    "# Specify the directory containing your txt files\n",
    "directory = \"2015-Country-Manifestos\\Spain\"\n",
    "process_files(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the pipelines\n",
    "translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-xx-en\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def translate_to_english(text):\n",
    "    # Translate the text to English\n",
    "    translated_text = translation_pipeline(text, max_length=512)[0]['translation_text']\n",
    "    return translated_text\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    # Perform sentiment analysis\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    return result\n",
    "\n",
    "def process_files(directory):\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    \n",
    "    for file in files:\n",
    "        filepath = os.path.join(directory, file)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            print(f\"Processing '{file}'...\")\n",
    "            \n",
    "            # Translate to English\n",
    "            english_text = translate_to_english(text)\n",
    "            sentiment = sentiment_analysis(english_text)\n",
    "            print(f\"Sentiment of '{file}': {sentiment['label']} with a score of {sentiment['score']}\\n\")\n",
    "\n",
    "# Specify the directory containing your txt files\n",
    "#directory = \"2015-Country-Manifestos\\Spain\"\n",
    "#process_files(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print(f\"Processing '{file}'...\")\n",
    "    english_text = translate_to_english(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sebgr\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-es-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "c:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\easynmt\\EasyNMT.py:449: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  sentences = list(re.findall(u'[^!?。\\.]+[!?。\\.]*', text, flags=re.U))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\easynmt\\EasyNMT.py:149\u001b[0m, in \u001b[0;36mEasyNMT.translate\u001b[1;34m(self, documents, target_lang, source_lang, show_progress_bar, beam_size, batch_size, perform_sentence_splitting, paragraph_split, sentence_splitter, document_language_detection, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m method_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [documents[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m ids]\n\u001b[0;32m    148\u001b[0m method_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_lang\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lng\n\u001b[1;32m--> 149\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, translated_sentences \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids, translated):\n\u001b[0;32m    151\u001b[0m     output[idx] \u001b[38;5;241m=\u001b[39m translated_sentences\n",
      "File \u001b[1;32mc:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\easynmt\\EasyNMT.py:181\u001b[0m, in \u001b[0;36mEasyNMT.translate\u001b[1;34m(self, documents, target_lang, source_lang, show_progress_bar, beam_size, batch_size, perform_sentence_splitting, paragraph_split, sentence_splitter, document_language_detection, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     sent2doc\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(splitted_sentences))\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m#logger.info(\"Sentence splitting done after: {:.2f} sec\".format(time.time() - start_time))\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m#logger.info(\"Translate {} sentences\".format(len(splitted_sentences)))\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m translated_sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplitted_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Merge sentences back to documents\u001b[39;00m\n\u001b[0;32m    184\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\easynmt\\EasyNMT.py:278\u001b[0m, in \u001b[0;36mEasyNMT.translate_sentences\u001b[1;34m(self, sentences, target_lang, source_lang, show_progress_bar, beam_size, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(iterator, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(sentences)\u001b[38;5;241m/\u001b[39mscale, unit_scale\u001b[38;5;241m=\u001b[39mscale, smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_idx \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m--> 278\u001b[0m     output\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_sorted\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m#Restore original sorting of sentences\u001b[39;00m\n\u001b[0;32m    281\u001b[0m output \u001b[38;5;241m=\u001b[39m [output[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "File \u001b[1;32mc:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\easynmt\\models\\OpusMT.py:49\u001b[0m, in \u001b[0;36mOpusMT.translate_sentences\u001b[1;34m(self, sentences, source_lang, target_lang, device, beam_size, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m     inputs[key] \u001b[38;5;241m=\u001b[39m inputs[key]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 49\u001b[0m     translated \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     output \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(t, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m translated]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\transformers\\generation\\utils.py:1626\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1619\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1620\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1621\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   1622\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1623\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1624\u001b[0m     )\n\u001b[0;32m   1625\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1637\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE:\n\u001b[0;32m   1642\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1643\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32mc:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\transformers\\generation\\utils.py:3143\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, sequential, **model_kwargs)\u001b[0m\n\u001b[0;32m   3139\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3140\u001b[0m     outputs, model_kwargs, is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder\n\u001b[0;32m   3141\u001b[0m )\n\u001b[0;32m   3142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3143\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_temporary_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_key_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\n\u001b[0;32m   3145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate \u001b[38;5;129;01mand\u001b[39;00m output_scores:\n\u001b[0;32m   3148\u001b[0m     beam_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((beam_indices[beam_idx[i]] \u001b[38;5;241m+\u001b[39m (beam_idx[i],) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(beam_indices))))\n",
      "File \u001b[1;32mc:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\transformers\\generation\\utils.py:2805\u001b[0m, in \u001b[0;36mGenerationMixin._temporary_reorder_cache\u001b[1;34m(self, past_key_values, beam_idx)\u001b[0m\n\u001b[0;32m   2803\u001b[0m \u001b[38;5;66;03m# Exception 1: code path for models using the legacy cache format\u001b[39;00m\n\u001b[0;32m   2804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(past_key_values, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m-> 2805\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2806\u001b[0m \u001b[38;5;66;03m# Exception 2: models with different cache formats. These are limited to `DynamicCache` until their\u001b[39;00m\n\u001b[0;32m   2807\u001b[0m \u001b[38;5;66;03m# cache format is standardized, to avoid adding complexity to the codebase.\u001b[39;00m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbloom\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_class \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgptbigcode\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_class:\n",
      "File \u001b[1;32mc:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1486\u001b[0m, in \u001b[0;36mMarianMTModel._reorder_cache\u001b[1;34m(past_key_values, beam_idx)\u001b[0m\n\u001b[0;32m   1482\u001b[0m reordered_past \u001b[38;5;241m=\u001b[39m ()\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past \u001b[38;5;129;01min\u001b[39;00m past_key_values:\n\u001b[0;32m   1484\u001b[0m     \u001b[38;5;66;03m# cached cross_attention states don't have to be reordered -> they are always the same\u001b[39;00m\n\u001b[0;32m   1485\u001b[0m     reordered_past \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1486\u001b[0m         \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpast_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1487\u001b[0m         \u001b[38;5;241m+\u001b[39m layer_past[\u001b[38;5;241m2\u001b[39m:],\n\u001b[0;32m   1488\u001b[0m     )\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reordered_past\n",
      "File \u001b[1;32mc:\\Users\\sebgr\\.conda\\envs\\Uni\\Lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1486\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1482\u001b[0m reordered_past \u001b[38;5;241m=\u001b[39m ()\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past \u001b[38;5;129;01min\u001b[39;00m past_key_values:\n\u001b[0;32m   1484\u001b[0m     \u001b[38;5;66;03m# cached cross_attention states don't have to be reordered -> they are always the same\u001b[39;00m\n\u001b[0;32m   1485\u001b[0m     reordered_past \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1486\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\u001b[43mpast_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m past_state \u001b[38;5;129;01min\u001b[39;00m layer_past[:\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m   1487\u001b[0m         \u001b[38;5;241m+\u001b[39m layer_past[\u001b[38;5;241m2\u001b[39m:],\n\u001b[0;32m   1488\u001b[0m     )\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reordered_past\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepath = '2015-Country-Manifestos/Spain/33020.txt'\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print(model.translate(text, target_lang='en'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(x):\n",
    "    try:\n",
    "        blob=TextBlob(x)\n",
    "        return (str(blob.translate(to = 'en')))\n",
    "    except: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easynmt import EasyNMT\n",
    "model = EasyNMT('opus-mt')\n",
    "\n",
    "#Translate a single sentence to German\n",
    "print(model.translate('This is a sentence we want to translate to German', target_lang='de'))\n",
    "\n",
    "#Translate several sentences to German\n",
    "sentences = ['You can define a list with sentences.',\n",
    "             'All sentences are translated to your target language.',\n",
    "             'Note, you could also mix the languages of the sentences.']\n",
    "print(model.translate(sentences, target_lang='de'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sie können eine Liste mit Sätzen definieren.', 'Alle Sätze werden in Ihre Zielsprache übersetzt.', 'Beachten Sie, Sie können auch die Sprachen der Sätze mischen.']\n"
     ]
    }
   ],
   "source": [
    "#Translate several sentences to German\n",
    "sentences = ['You can define a list with sentences.',\n",
    "             'All sentences are translated to your target language.',\n",
    "             'Note, you could also mix the languages of the sentences.']\n",
    "print(model.translate(sentences, target_lang='de'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(sentence):\n",
    "\tsid_obj = SentimentIntensityAnalyzer()\n",
    "\t# polarity_scores method of SentimentIntensityAnalyzer\n",
    "\t# object gives a sentiment dictionary.\n",
    "\t# which contains pos, neg, neu, and compound scores.\n",
    "\tsentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "\t\n",
    "\tprint(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "\tprint(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "\tprint(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "\tprint(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    "\n",
    "\tprint(\"Sentence Overall Rated As\", end = \" \")\n",
    "\n",
    "\t# decide sentiment as positive, negative and neutral\n",
    "\tif sentiment_dict['compound'] >= 0.05 :\n",
    "\t\tprint(\"Positive\")\n",
    "\n",
    "\telif sentiment_dict['compound'] <= - 0.05 :\n",
    "\t\tprint(\"Negative\")\n",
    "\n",
    "\telse :\n",
    "\t\tprint(\"Neutral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2015-Country-Manifestos\\United Kingdom\\51110.txt\", 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        print(sentiment_scores(line))\n",
    "\n",
    "\n",
    "    #return translated_lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
